{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_tokenizer import MultiTokenizer, PretrainedTokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `split_text=True` to split text into sentences which helps to improve the \n",
    "language detection accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_tokenizers = [\n",
    "    PretrainedTokenizers.ENGLISH,\n",
    "    PretrainedTokenizers.CHINESE,\n",
    "    PretrainedTokenizers.HINDI,\n",
    "]\n",
    "fallback_tokenizer = PretrainedTokenizers.ENGLISH\n",
    "\n",
    "tokenizer = MultiTokenizer(lang_tokenizers, fallback_tokenizer, split_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<EN>', (0, 1)),\n",
       " ('The', (1, 4)),\n",
       " ('Ġcat', (4, 8)),\n",
       " ('Ġis', (8, 11)),\n",
       " ('Ġcute', (11, 16)),\n",
       " ('.', (16, 17)),\n",
       " ('</EN>', (15, 16)),\n",
       " (' ', (16, 17)),\n",
       " ('<ZH>', (17, 18)),\n",
       " ('çĮ«å¾Īåı¯çĪ±', (18, 22)),\n",
       " ('.', (22, 23)),\n",
       " ('</ZH>', (21, 22)),\n",
       " (' ', (22, 23)),\n",
       " ('<HI>', (23, 24)),\n",
       " ('à¤¬', (24, 25)),\n",
       " ('à¤¿', (25, 26)),\n",
       " ('à¤²', (26, 27)),\n",
       " ('à¥į', (27, 28)),\n",
       " ('à¤²', (28, 29)),\n",
       " ('à¥Ģ', (29, 30)),\n",
       " ('Ġà¤¬à¤¹', (30, 33)),\n",
       " ('à¥ģ', (33, 34)),\n",
       " ('à¤¤', (34, 35)),\n",
       " ('Ġà¤ª', (35, 37)),\n",
       " ('à¥į', (37, 38)),\n",
       " ('à¤¯', (38, 39)),\n",
       " ('à¤¾', (39, 40)),\n",
       " ('à¤°', (40, 41)),\n",
       " ('à¥Ģ', (41, 42)),\n",
       " ('Ġà¤¹', (42, 44)),\n",
       " ('à¥Ī.', (44, 46)),\n",
       " ('</HI>', (44, 45)),\n",
       " (' නර්තනය ඉතා ආදරේ.', (45, 62))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence = \"The cat is cute. 猫很可爱. बिल्ली बहुत प्यारी है.\"\n",
    "# sentence = \"Translate this hindi sentence to english - बिल्ली बहुत प्यारी है.\"\n",
    "sentence = \"The cat is cute. 猫很可爱. बिल्ली बहुत प्यारी है. නර්තනය ඉතා ආදරේ.\"\n",
    "tokenizer.pre_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(sentence)\n",
    "ids = tokenizer.encode(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<EN>', 'The', 'Ġcat', 'Ġis', 'Ġcute', '.', '</EN>', 'Ġ', '<ZH>', 'çĮ', '«', 'å¾Ī', 'åı¯', 'çĪ', '±', '.', '</ZH>', 'Ġ', '<HI>', 'à¤¬', 'à¤¿', 'à¤²', 'à¥į', 'à¤²', 'à¥Ģ', 'Ġà¤¬à¤¹', 'à¥ģ', 'à¤¤', 'Ġà¤ª', 'à¥į', 'à¤¯', 'à¤¾', 'à¤°', 'à¥Ģ', 'Ġà¤¹', 'à¥Ī.', '</HI>', 'Ġ', 'à', '¶', '±', 'à', '¶', '»', 'à', '·', 'Ĭ', 'à', '¶', 'Ń', 'à', '¶', '±', 'à', '¶', 'º', 'Ġ', 'à', '¶', 'ī', 'à', '¶', 'Ń', 'à', '·', 'ı', 'Ġ', 'à', '¶', 'Ĩ', 'à', '¶', '¯', 'à', '¶', '»', 'à', '·', 'ļ', '.']\n",
      "[3, 383, 714, 416, 2065, 24, 4, 231, 7, 1512, 115, 9849, 368, 439, 120, 24, 8, 231, 9, 329, 277, 285, 282, 285, 273, 342, 286, 283, 294, 282, 292, 270, 272, 273, 287, 919, 10, 231, 167, 125, 120, 167, 125, 130, 167, 126, 243, 167, 125, 266, 167, 125, 120, 167, 125, 129, 231, 167, 125, 242, 167, 125, 266, 167, 126, 248, 231, 167, 125, 239, 167, 125, 118, 167, 125, 130, 167, 126, 259, 24]\n"
     ]
    }
   ],
   "source": [
    "print(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded String: The cat is cute. 猫很可爱. बिल्ली बहुत प्यारी है. නර්තනය ඉතා ආදරේ.\n",
      "Original String: The cat is cute. 猫很可爱. बिल्ली बहुत प्यारी है. නර්තනය ඉතා ආදරේ.\n"
     ]
    }
   ],
   "source": [
    "print(\"Decoded String:\", tokenizer.decode(ids))\n",
    "print(\"Original String:\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "aya_tokenizer = AutoTokenizer.from_pretrained(\"CohereForAI/aya-23-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "['The', 'Ġcat', 'Ġis', 'Ġcute', '.', 'Ġ', 'çĮ«', 'å¾Ī', 'åı¯', 'çĪ±', '.', 'Ġà¤¬', 'à¤¿', 'à¤²', 'à¥į', 'à¤²', 'à¥Ģ', 'Ġà¤¬à¤¹', 'à¥ģ', 'à¤¤', 'Ġà¤ª', 'à¥į', 'à¤¯', 'à¤¾', 'à¤°', 'à¥Ģ', 'Ġà¤¹', 'à¥Ī.', 'Ġà¶', '±', 'à¶', '»', 'à·Ĭ', 'à¶', 'Ń', 'à¶', '±', 'à¶', 'º', 'Ġà¶', 'ī', 'à¶', 'Ń', 'à·', 'ı', 'Ġà¶', 'Ĩ', 'à¶', '¯', 'à¶', '»', 'à·', 'ļ', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = aya_tokenizer.tokenize(sentence)\n",
    "print(len(tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255029"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aya_tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 2162, 8592, 1801, 44997, 21, 228, 50826, 14441, 4996, 15069, 21, 5144, 2337, 2973, 2173, 2973, 2491, 34215, 3695, 2560, 3468, 2173, 2857, 2054, 2169, 2491, 3372, 208810, 187055, 117, 77103, 127, 200914, 77103, 263, 77103, 117, 77103, 126, 187055, 239, 77103, 263, 71791, 245, 187055, 236, 77103, 115, 77103, 127, 71791, 256, 21]\n"
     ]
    }
   ],
   "source": [
    "ids = aya_tokenizer.encode(sentence)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded String: <BOS_TOKEN>The cat is cute. 猫很可爱. बिल्ली बहुत प्यारी है. නර්තනය ඉතා ආදරේ.\n",
      "Original String: The cat is cute. 猫很可爱. बिल्ली बहुत प्यारी है. නර්තනය ඉතා ආදරේ.\n"
     ]
    }
   ],
   "source": [
    "print(\"Decoded String:\", aya_tokenizer.decode(ids))\n",
    "print(\"Original String:\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243 μs ± 29.7 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit aya_tokenizer.encode(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "892 μs ± 24.6 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit tokenizer.encode(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi-tokenizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
